{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c00165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc771d",
   "metadata": {},
   "source": [
    "Чтобы заработало локально, изменить в config.py: POSTGRES_HOST = \"db\" -> POSTGRES_HOST = \"localhost\"\n",
    "и путь в SPARK_JARS_DIR заменить на локальный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0aa91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.readers import read_csv_to_spark\n",
    "from utils.writers import upsert_spark_df_to_postgres\n",
    "from clients.spark_client import create_spark_session\n",
    "from utils.tools import transform_rd_dfs\n",
    "from etl.stages import sync_rd_tables\n",
    "from config import (\n",
    "    dwh_db_name,\n",
    "    rd_schema_name,\n",
    "    rd_sql_filename,\n",
    "    task_2_files_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cde7813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/15 19:13:34 WARN Utils: Your hostname, maxp-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/07/15 19:13:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/15 19:13:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/15 19:13:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e0f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 19:24:46,057 [INFO] etl.stages: === Начало процесса синхронизации таблиц ===\n",
      "2025-07-15 19:24:46,063 [INFO] etl.stages: Начало: 2025-07-15 19:24:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 19:24:51,084 [INFO] db_utils.check_postges: Проверка/создание БД 'dwh', схемы 'rd', и таблиц из 'db_utils/rd_tables.sql'\n",
      "2025-07-15 19:24:51,109 [INFO] db_utils.check_postges: Проверка существования базы данных 'dwh'\n",
      "2025-07-15 19:24:51,116 [INFO] db_utils.check_postges: База данных 'dwh' уже существует.\n",
      "2025-07-15 19:24:51,143 [INFO] db_utils.check_postges: Создаю схему 'rd' в базе 'dwh' (если она отсутствует)\n",
      "2025-07-15 19:24:51,153 [INFO] db_utils.check_postges: Схема 'rd' готова.\n",
      "2025-07-15 19:24:51,191 [INFO] db_utils.check_postges: Читаю SQL из файла 'db_utils/rd_tables.sql'\n",
      "2025-07-15 19:24:51,201 [INFO] db_utils.check_postges: Выполняю SQL для создания таблиц\n",
      "2025-07-15 19:24:51,215 [INFO] db_utils.check_postges: Таблицы успешно созданы.\n",
      "2025-07-15 19:24:51,223 [INFO] etl.stages: Начало обновления таблиц в схеме 'rd' БД 'dwh'\n",
      "2025-07-15 19:24:54,482 [INFO] etl.stages: Данные из csv успешно загружены: ['deal_info', 'product_info']\n",
      "2025-07-15 19:24:54,484 [INFO] etl.stages: Truncate таблицы rd.deal_info перед полной загрузкой\n",
      "TRUNCATE TABLE rd.deal_info\n",
      "2025-07-15 19:24:54,512 [INFO] etl.stages: Таблица успешно очищена.\n",
      "2025-07-15 19:24:54,516 [INFO] etl.stages: Вставка данных в deal_info\n",
      "2025-07-15 19:24:54,522 [INFO] etl.stages: Загрузка данных в {table_name} завершена.\n",
      "2025-07-15 19:24:54,529 [INFO] etl.stages: Truncate таблицы rd.product_info перед полной загрузкой\n",
      "TRUNCATE TABLE rd.product_info\n",
      "2025-07-15 19:24:54,591 [INFO] etl.stages: Таблица успешно очищена.\n",
      "2025-07-15 19:24:54,597 [INFO] etl.stages: Вставка данных в product_info\n",
      "2025-07-15 19:24:54,617 [INFO] etl.stages: Загрузка данных в {table_name} завершена.\n",
      "2025-07-15 19:24:54,622 [INFO] etl.stages: Обновление всех таблиц завершено за 3.39 секунд.\n",
      "2025-07-15 19:24:54,628 [INFO] etl.stages: Окончание: 2025-07-15 19:24:54\n",
      "2025-07-15 19:24:54,644 [INFO] etl.stages: Длительность: 0:00:08.565059\n",
      "2025-07-15 19:24:56,515 [INFO] db_utils.check_postges: Проверка/создание БД 'project', схемы 'logs', и таблиц из 'db_utils/log_table.sql'\n",
      "2025-07-15 19:24:56,537 [INFO] db_utils.check_postges: Проверка существования базы данных 'project'\n",
      "2025-07-15 19:24:56,544 [INFO] db_utils.check_postges: База данных 'project' уже существует.\n",
      "2025-07-15 19:24:56,601 [INFO] db_utils.check_postges: Создаю схему 'logs' в базе 'project' (если она отсутствует)\n",
      "2025-07-15 19:24:56,607 [INFO] db_utils.check_postges: Схема 'logs' готова.\n",
      "2025-07-15 19:24:56,648 [INFO] db_utils.check_postges: Читаю SQL из файла 'db_utils/log_table.sql'\n",
      "2025-07-15 19:24:56,655 [INFO] db_utils.check_postges: Выполняю SQL для создания таблиц\n",
      "2025-07-15 19:24:56,671 [INFO] db_utils.check_postges: Таблицы успешно созданы.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:====================================>                      (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-15 19:24:59,771 [INFO] etl.stages: === Конец процесса ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sync_rd_tables(db_name=dwh_db_name, \n",
    "               schema_name=rd_schema_name, \n",
    "               sql_filename=rd_sql_filename, \n",
    "               raw_files_info=task_2_files_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6d000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
